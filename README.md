# Large-Model-Uncertainty

## LLM (Large Language Model) Uncertainty
---
### Uncertainty Quantification and Calibration
|Year|Venue|Title|Code|
|:-:|:-:|-|:-:|
|2024|ICLR|[Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs](https://arxiv.org/abs/2306.13063)|[code](https://github.com/MiaoXiong2320/llm-uncertainty)|
|2024|ICLR|[INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection](https://arxiv.org/pdf/2402.03744)|N.A.|
|2024|ICML|[Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling](https://arxiv.org/abs/2311.08718)|[code](https://github.com/UCSB-NLP-Chang/llm_uncertainty)|
|2024|ICML|[Can AI Assistants Know What They Don’t Know?](https://proceedings.mlr.press/v235/cheng24i.html)|[code](https://github.com/OpenMOSS/Say-I-Dont-Know)|
|2024|ACL|[When to Trust LLMs: Aligning Confidence with Response Quality](https://arxiv.org/abs/2404.17287)|N.A.|
|2024|ACL|[Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models](https://arxiv.org/abs/2307.01379)|[code](https://github.com/jinhaoduan/SAR)|
|2024|ACL|[Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness](https://aclanthology.org/2024.acl-long.283.pdf)|N.A.|
|2024|NAACL|[Uncertainty Quantification for In-Context Learning of Large Language Models](https://aclanthology.org/2024.naacl-long.184/)|[code](https://github.com/lingchen0331/UQ_ICL)|
|2024|NAACL|[A Study on the Calibration of In-context Learning](https://arxiv.org/abs/2312.04021)|[code](https://github.com/hlzhang109/icl-calibration)|
|2024|NAACL|[R-Tuning: Instructing Large Language Models to Say ‘I Don’t Know’](https://aclanthology.org/2024.naacl-long.394/)|[code](https://github.com/shizhediao/R-Tuning)|
|2024|TMLR|[Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models](https://arxiv.org/abs/2305.19187)|[code](https://github.com/zlin7/UQ-NLG)|
|2024|UAI|[CSS: Contrastive Semantic Similarity for Uncertainty Quantification of LLMs](https://arxiv.org/abs/2406.03158)|[code](https://github.com/AoShuang92/css_uq_llms)|
|2024|arxiv|[A Survey of Confidence Estimation and Calibration in Large Language Models](https://arxiv.org/abs/2311.08298)|N.A.|
|2024|arxiv|[SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales](https://arxiv.org/abs/2405.20974)|[code](https://github.com/xu1868/SaySelf)|
|2024|arxiv|[Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models](https://arxiv.org/abs/2407.11282)|[code](https://github.com/qcznlp/uncertainty_attack)|
|2024|arxiv|[Benchmarking LLMs via Uncertainty Quantification](https://arxiv.org/abs/2401.12794)|[code](https://github.com/smartyfh/LLM-Uncertainty-Bench)|
|2024|arxiv|[To Believe or Not to Believe Your LLM](https://arxiv.org/abs/2406.02543)|N.A.|
|2024|arxiv|[Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs](https://arxiv.org/abs/2406.15927)|N.A.|
|2023|EMNLP Findings|[Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback](https://arxiv.org/abs/2305.14975)|N.A.|
|2023|ICLR|[Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation](https://arxiv.org/abs/2302.09664)|N.A.|
|2023|ICLR|[Calibrating Sequence Likelihood Improves Conditional Language Generation](https://openreview.net/forum?id=0qSOodKmJaN)|N.A.|
|2023|NeurIPS workshop|[Self-Evaluation Improves Selective Generation in Large Language Models](https://proceedings.mlr.press/v239/ren23a.html)|N.A.|
|2023|arxiv|[SLiC-HF: Sequence Likelihood Calibration with Human Feedback](https://arxiv.org/abs/2305.10425)|N.A.|
|2022|TMLR|[Teaching Models to Express Their Uncertainty in Words](https://arxiv.org/abs/2205.14334)|N.A.|

### Model Cascade
|Year|Venue|Title|Code|
|:-:|:-:|-|:-:|
|2024|ICLR|[Language Model Cascades: Token-level uncertainty and beyond](https://arxiv.org/abs/2404.10136)|N.A.|
|2024|ICLR|[Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning](https://arxiv.org/pdf/2310.03094.pdf)|[code](https://github.com/MurongYue/LLM_MoT_cascade)|
|2023|NeurIPS|[When Does Confidence-Based Cascade Deferral Suffice?](https://arxiv.org/abs/2307.02764)|N.A.|
|2024|arxiv|[Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement](https://arxiv.org/abs/2407.18370)|[code](https://github.com/jaehunjung1/cascaded-selective-evaluation)|
|2024|arixv|[Model Cascading for Code: Reducing Inference Costs with Model Cascading for LLM Based Code Generation](https://arxiv.org/abs/2405.15842)|N.A.|

### Selective Prediction
|Year|Venue|Title|Code|
|:-:|:-:|-|:-:|
|2024|arxiv|[Know Your Limits: A Survey of Abstention in Large Language Models](https://arxiv.org/abs/2407.18418)|N.A.|
|2024|ICML workshop|[Towards Human-AI Collaboration in Healthcare: Guided Deferral Systems with Large Language Models](https://arxiv.org/abs/2406.07212)|N.A.|
|2023|EMNLP Findings|[Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs](https://arxiv.org/abs/2310.11689)|N.A.|

## MLLM (Multi-modal Large Language Model) Uncertainty
---
|Year|Venue|Title|Code|
|:-:|:-:|-|:-:|
|2024|CVPR|[Consistency and Uncertainty: Identifying Unreliable Responses From Black-Box Vision-Language Models for Selective Visual Question Answering](https://openaccess.thecvf.com/content/CVPR2024/papers/Khan_Consistency_and_Uncertainty_Identifying_Unreliable_Responses_From_Black-Box_Vision-Language_Models_CVPR_2024_paper.pdf)|N.A.|
|2024|NAACL workshop|[Overconfidence is Key: Verbalized Uncertainty Evaluation in Large Language and Vision-Language Models](https://arxiv.org/abs/2405.02917)|N.A.|
|2024|arxiv|[Uncertainty-Aware Evaluation for Vision-Language Models](https://arxiv.org/abs/2402.14418)|[code](https://github.com/EnSec-AI/VLM-Uncertainty-Bench)|
|2024|arxiv|[Decompose and Compare Consistency: Measuring VLMs’ Answer Reliability via Task-Decomposition Consistency Comparison](http://arxiv.org/abs/2407.07840)|N.A.|
|2024|arxiv|[Certainly Uncertain: A Benchmark and Metric for Multimodal Epistemic and Aleatoric Awareness](https://arxiv.org/abs/2407.01942)|N.A.|
|2024|arxiv|[Selectively Answering Visual Questions](https://arxiv.org/abs/2406.00980)|N.A.|

## CLIP Uncertainty
---
|Year|Venue|Title|Code|
|:-:|:-:|-|:-:|
|2024|ICML|[An Empirical Study Into What Matters for Calibrating Vision–Language Models](https://arxiv.org/abs/2402.07417)|N.A.|
|2024|ICML|[Open-Vocabulary Calibration for Fine-tuned CLIP](https://arxiv.org/abs/2402.04655)|[code](https://github.com/ml-stat-Sustech/CLIP_Calibration)|
|2023|NeurIPS|[A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP)](https://arxiv.org/abs/2402.07410)|N.A.|
|2024|arxiv|[Towards Calibrated Robust Fine-Tuning of Vision-Language Model](https://arxiv.org/abs/2311.01723)|[code](https://anonymous.4open.science/r/carot2024-BB43/README.md)|
